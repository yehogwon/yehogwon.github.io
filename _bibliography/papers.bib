---
---

@string{CVPR = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)}}
@string{ECCV = {European Conference on Computer Vision (<b>ECCV</b>)}}
@string{ICCV = {International Conference on Computer Vision (<b>ICCV</b>)}}
@string{NEURIPS = {Conference on Neural Information Processing Systems (<b>NeurIPS</b>)}}
@string{IJCV = {International Journal of Computer Vision (<b>IJCV</b>)}}
@string{ICRA = {IEEE International Conference on Robotics and Automation (<b>ICRA</b>)}}
@string{ICML = {International Conference on Machine Learning (<b>ICML</b>)}}
@string{TMLR = {Transactions on Machine Learning Research (<b>TMLR</b>)}}
@string{arXiv = {arXiv preprint}}


@article{lee2025robust,
    title={Robust Promptable Video Object Segmentation},
    author={Sohyun Lee and Yeho Gwon and Lukas Hoyer and Konrad Schindler and Christos Sakaridis and Suha Kwak},
    abstract = {The performance of promptable video object segmentation (PVOS) models substantially degrades under input corruptions, which prevents PVOS deployment in safety-critical domains. This paper offers the first comprehensive study on robust PVOS (RobustPVOS). We first construct a new, comprehensive benchmark with two real-world evaluation datasets of 351 video clips and more than 2,500 object masks under real-world adverse conditions. At the same time, we generate synthetic training data by applying diverse and temporally varying corruptions to existing VOS datasets. Moreover, we present a new RobustPVOS method, dubbed Memory-object-conditioned Gated-rank Adaptation (MoGA). The key to successfully performing RobustPVOS is two-fold: effectively handling object-specific degradation and ensuring temporal consistency in predictions. MoGA leverages object-specific representations maintained in memory across frames to condition the robustification process, which allows the model to handle each tracked object differently in a temporally consistent way. Extensive experiments on our benchmark validate MoGA's efficacy, showing consistent and significant improvements across diverse corruption types on both synthetic and real-world datasets, establishing a strong baseline for future RobustPVOS research. Our benchmark will be made publicly available.},
    year={2025},
    booktitle={Under Review},
    selected={true},
}

@article{lee2025garasam,
    title={{GaRA-SAM}: Robustifying Segment Anything Model with Gated-Rank Adaptation}, 
    author={Sohyun Lee and Yeho Gwon and Lukas Hoyer and Suha Kwak},
    year={2025},
    eprint={2506.02882},
    journal=NeurIPS,
    abbr={NeurIPS},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2506.02882},
    project={https://sohyun-l.github.io/garasam_project_page/},
    code={https://github.com/sohyun-l/GaRA-SAM},
    selected={true},
    img_path={assets/img/publication_preview/lee2025garasam.png},
    abstract={Improving robustness of the Segment Anything Model (SAM) to input degradations is critical for its deployment in high-stakes applications such as autonomous driving and robotics. Our approach to this challenge prioritizes three key aspects: first, parameter efficiency to maintain the inherent generalization capability of SAM; second, fine-grained and input-aware robustification to precisely address the input corruption; and third, adherence to standard training protocols for ease of training. To this end, we propose gated-rank adaptation (GaRA). GaRA introduces lightweight adapters into intermediate layers of the frozen SAM, where each adapter dynamically adjusts the effective rank of its weight matrix based on the input by selectively activating (rank-1) components of the matrix using a learned gating module. This adjustment enables fine-grained and input-aware robustification without compromising the generalization capability of SAM. Our model, GaRA-SAM, significantly outperforms prior work on all robust segmentation benchmarks. In particular, it surpasses the previous best IoU score by up to 21.3%p on ACDC, a challenging real corrupted image dataset.},
    html={https://arxiv.org/abs/2506.02882},
    additional_info2={ICCV 2025 Workshop on Building Foundation Models You Can Trust <i>(Oral)</i>}
}

@article{gwon2025enhancing,
    title={Enhancing Cost Efficiency in Active Learning with Candidate Set Query}, 
    author={Yeho Gwon and Sehyun Hwang and Hoyoung Kim and Jungseul Ok and Suha Kwak},
    year={2025},
    eprint={2502.06209},
    journal=TMLR,
    abbr={TMLR},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2502.06209}, 
    equal_contrib={true},
    selected={true},
    img_path={assets/img/publication_preview/gwon2025enhancing.png},
    code={https://github.com/yehogwon/csq-al},
    project={csq-al},
    abstract={This paper introduces a cost-efficient active learning (AL) framework for classification, featuring a novel query design called candidate set query. Unlike traditional AL queries requiring the oracle to examine all possible classes, our method narrows down the set of candidate classes likely to include the ground-truth class, significantly reducing the search space and labeling cost. Moreover, we leverage conformal prediction to dynamically generate small yet reliable candidate sets, adapting to model enhancement over successive AL rounds. To this end, we introduce an acquisition function designed to prioritize data points that offer high information gain at lower cost. Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the effectiveness and scalability of our framework. Notably, it reduces labeling cost by 48% on ImageNet64x64.},
    html={https://arxiv.org/abs/2502.06209}
}
